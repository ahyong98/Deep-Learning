{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "best_KOBERT",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ece5f7bc097640358632e681b13deff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2146ba5ff0eb4f4f9597ad9cc5f618d4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a20d986d4dac43188c72e4ce696906ea",
              "IPY_MODEL_ce6a2b06bb574bf0a9e9d41eac9dc0b0"
            ]
          }
        },
        "2146ba5ff0eb4f4f9597ad9cc5f618d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a20d986d4dac43188c72e4ce696906ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_189cee309ae64ea193eb6b30b519f3a9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 371391,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 371391,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a7a02f6b3aa4430d9ec0fd9476af0841"
          }
        },
        "ce6a2b06bb574bf0a9e9d41eac9dc0b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_07a2eb8b35a74b2d9499f62ddb99f6de",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 371k/371k [00:03&lt;00:00, 116kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_58e48b122d17491794dd4429494a356e"
          }
        },
        "189cee309ae64ea193eb6b30b519f3a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a7a02f6b3aa4430d9ec0fd9476af0841": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "07a2eb8b35a74b2d9499f62ddb99f6de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "58e48b122d17491794dd4429494a356e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1f8e84c00d8a458b88a13468ff4b45ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3f56bf234396496cb8f3e75ab08164dc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3e0c5f33cfb940a8a7e0081666dafdbd",
              "IPY_MODEL_da25be21f0374bc988a8262e42fc860a"
            ]
          }
        },
        "3f56bf234396496cb8f3e75ab08164dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3e0c5f33cfb940a8a7e0081666dafdbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7709765688674479bb6b7e6094ef3f64",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 77779,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 77779,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_89be3f07490345708d64513881fb7984"
          }
        },
        "da25be21f0374bc988a8262e42fc860a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bc4e794f5c1a4cc78a33ec6d3ac2433e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 77.8k/77.8k [00:02&lt;00:00, 31.0kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bba0b85609a04a4687f6633e04efa983"
          }
        },
        "7709765688674479bb6b7e6094ef3f64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "89be3f07490345708d64513881fb7984": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bc4e794f5c1a4cc78a33ec6d3ac2433e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bba0b85609a04a4687f6633e04efa983": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GV6p8TqJVPrL"
      },
      "source": [
        "## 환경설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKD9HTYxJMgC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "3c3a9a9c-beeb-4ba3-d6c3-cf5b30af273b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFqpKEmTJRuK"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/딥러닝/감성분석')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlIyZKObuQMY"
      },
      "source": [
        "# 파일 압축 풀기\n",
        "import zipfile\n",
        "         \n",
        "fantasy_zip = zipfile.ZipFile('/content/drive/My Drive/딥러닝/감성분석/2020-1-deeplearning-final.zip')\n",
        "fantasy_zip.extractall('/content/drive/My Drive/딥러닝/감성분석')\n",
        " \n",
        "fantasy_zip.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0sZYfQsu111",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "fee38639-2db5-40f9-da41-51d565377f7c"
      },
      "source": [
        "# 패키지 임포트\n",
        "\n",
        "import pandas as pd\n",
        "!pip install konlpy\n",
        "from konlpy.tag import Okt\n",
        "import re\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, GRU, LSTM, SpatialDropout1D, Dropout\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.7.5)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from konlpy) (0.4.3)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.18.5)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (3.8.0)\n",
            "Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.12.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (2020.4.5.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RpQ1SM2whmO"
      },
      "source": [
        "## 데이터"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4d52jX8ukOn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "f2f657d3-4b2e-4b38-f7c6-e2e0c48db0a1"
      },
      "source": [
        "# loading data\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>rating</th>\n",
              "      <th>raw</th>\n",
              "      <th>chr_token</th>\n",
              "      <th>nn_token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>R024795</td>\n",
              "      <td>1</td>\n",
              "      <td>1982년도에 이런영화가 제작이 됐다는거 자체가 놀라운거라고. 재미없다고 씨부리는것...</td>\n",
              "      <td>1 9 8 2 년 도 에 이 런 영 화 가 제 작 이 됐 다 는 거 자 체 가 놀 ...</td>\n",
              "      <td>년도 영화 제작 거 자체 거 씨 것 엄매 뱃속 정자 난자 전 리를리감 독 전구 에디...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>R068332</td>\n",
              "      <td>1</td>\n",
              "      <td>핸드핼드기법을 적절히 써가며 현실과 환상의 대비를 확연하게 보여주는 영화</td>\n",
              "      <td>핸 드 핼 드 기 법 을 적 절 히 써 가 며 현 실 과 환 상 의 대 비 를 확 ...</td>\n",
              "      <td>핸드 핼드 기법 현실 환상 대비 영화</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>R027236</td>\n",
              "      <td>1</td>\n",
              "      <td>....엑스맨을 깔끔하게 정리를 했다는 자체가 이미 뛰어난 영화라는 것을 증명하고 있다</td>\n",
              "      <td>. . . . 엑 스 맨 을 깔 끔 하 게 정 리 를 했 다 는 자 체 가 이 미 ...</td>\n",
              "      <td>엑스맨 정리 자체 영화 것 증명</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>R050965</td>\n",
              "      <td>0</td>\n",
              "      <td>뭐 이런 쓰레기 영화가 다있냐 인물관계도 정리안해줘 줄거리도 없어 내용도 ㅉㅉ</td>\n",
              "      <td>뭐 이 런 쓰 레 기 영 화 가 다 있 냐 인 물 관 계 도 정 리 안 해 줘 줄 ...</td>\n",
              "      <td>쓰레기 영화 인물 관계 정리안 줄거리 내용 ㅉ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>R016317</td>\n",
              "      <td>0</td>\n",
              "      <td>육식공룡중에서 빌딩보다 더큰공룡이어딧노 ㅋㅋㅋㅋㅋㅋ</td>\n",
              "      <td>육 식 공 룡 중 에 서 빌 딩 보 다 더 큰 공 룡 이 어 딧 노 ㅋ ㅋ ㅋ ㅋ ㅋ ㅋ</td>\n",
              "      <td>육식 공 룡 중 빌딩 공룡 어딧노 ㅋ ㅋ</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ...                                           nn_token\n",
              "0  R024795  ...  년도 영화 제작 거 자체 거 씨 것 엄매 뱃속 정자 난자 전 리를리감 독 전구 에디...\n",
              "1  R068332  ...                               핸드 핼드 기법 현실 환상 대비 영화\n",
              "2  R027236  ...                                  엑스맨 정리 자체 영화 것 증명\n",
              "3  R050965  ...                          쓰레기 영화 인물 관계 정리안 줄거리 내용 ㅉ\n",
              "4  R016317  ...                             육식 공 룡 중 빌딩 공룡 어딧노 ㅋ ㅋ\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApXCujdDvqnb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "05bd409c-7ad9-4250-c6cd-e13fc82f991c"
      },
      "source": [
        "# class 분포 확인\n",
        "train.groupby(['rating'])['id'].size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "rating\n",
              "0    27255\n",
              "1    27255\n",
              "Name: id, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTJEnozuwQpl"
      },
      "source": [
        "# SKT : 한글 pre-trained BERT(KOBERT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AB7YtQ3rZzF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "outputId": "9ffcfb44-b3a2-4728-8bbe-746f07391e32"
      },
      "source": [
        "!pip install transformers # hugging face"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 15.5MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 23.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 43.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=72926c347f28b82d48982a5c83f4473604d2320afae4536203c699c775068e15\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRXvtgWrrdk5"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import *\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XeWU_2OsRJ3"
      },
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2hl88Wrlufp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "outputId": "6e9c8dfb-6be8-4c05-cc14-acf5d9419506"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>raw</th>\n",
              "      <th>chr_token</th>\n",
              "      <th>nn_token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>R009132</td>\n",
              "      <td>감독이 넘 제멋대로 자유분방(?)하게 만든거같다는...보구나서 느낌은 모지 이영화?</td>\n",
              "      <td>감 독 이 넘 제 멋 대 로 자 유 분 방 ( ? ) 하 게 만 든 거 같 다 는 ...</td>\n",
              "      <td>감독 자유 분방 느낌 모지 이영화</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>R045659</td>\n",
              "      <td>제작사와감독에게할복을요청하고싶다...일본영화의 최대망작!!!</td>\n",
              "      <td>제 작 사 와 감 독 에 게 할 복 을 요 청 하 고 싶 다 . . . 일 본 영 ...</td>\n",
              "      <td>제작사 감독 할복 요청 일본 영화 최대망작</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>R015196</td>\n",
              "      <td>드라마는 재미 있고 모든 출연자들 자알합니다만 단 한 사람 추잡스러워서 드라마 질을...</td>\n",
              "      <td>드 라 마 는 재 미 있 고 모 든 출 연 자 들 자 알 합 니 다 만 단 한 사 ...</td>\n",
              "      <td>드라마 재미 출연자 자 사람 잡 드라마 질 입술 성형 처음 때 지금 잡 서리</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>R063595</td>\n",
              "      <td>초등생인 첫째아이가 어렸을때 좋아했던 토마스.. 다시봐도 재밌다네요..둘째도 너무 ...</td>\n",
              "      <td>초 등 생 인 첫 째 아 이 가 어 렸 을 때 좋 아 했 던 토 마 스 . . 다 ...</td>\n",
              "      <td>초등 아이 때 토마스 다 아이 영화</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>R053340</td>\n",
              "      <td>중국을 보고싶다면 미모와 연기력을 다 갖춘 중국의 아이콘을 보고싶다면...</td>\n",
              "      <td>중 국 을 보 고 싶 다 면 미 모 와 연 기 력 을 다 갖 춘 중 국 의 아 이 ...</td>\n",
              "      <td>중국 미모 연기력 중국 아이콘</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13621</th>\n",
              "      <td>R002153</td>\n",
              "      <td>생각지도못한 반전영화 12년 전 영화인데도 최고</td>\n",
              "      <td>생 각 지 도 못 한 반 전 영 화 1 2 년 전 영 화 인 데 도 최 고</td>\n",
              "      <td>생각 전 영화 년 전 영화 최고</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13622</th>\n",
              "      <td>R029129</td>\n",
              "      <td>결말이 예측 되다가도 완전 빗나간 것 같다가 어느정도 예상에 접근하며 끝난 영화. ...</td>\n",
              "      <td>결 말 이 예 측 되 다 가 도 완 전 빗 나 간 것 같 다 가 어 느 정 도 예 ...</td>\n",
              "      <td>결말 예측 완전 것 정도 예상 접근 영화 흥미 몰입도 상당 영화 현실 것 고 설명 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13623</th>\n",
              "      <td>R021090</td>\n",
              "      <td>여태것 본 공포영화 중 젤 쓰레기 내용은 좋은데 감정적으로 대응하는 부분이 쓸대없이...</td>\n",
              "      <td>여 태 것 본 공 포 영 화 중 젤 쓰 레 기 내 용 은 좋 은 데 감 정 적 으 ...</td>\n",
              "      <td>태 것 공포 영화 중 젤 쓰레기 내용 감정 대응 부분 대 번 볼영</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13624</th>\n",
              "      <td>R013734</td>\n",
              "      <td>극장영화가 아니라 TV영화임! 이러한 2시간 영화를 찍을려고 해도 몇달동안 무더운 ...</td>\n",
              "      <td>극 장 영 화 가 아 니 라 T V 영 화 임 ! 이 러 한 2 시 간 영 화 를 ...</td>\n",
              "      <td>극장 영화 영화 시간 영화 달 동안 여름 밤 낮 새벽 잠 것 촬영 신인감독 신인 배...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13625</th>\n",
              "      <td>R050035</td>\n",
              "      <td>세 커플의 그저그런 사랑이야기인 것 같지만 사실 한 남자의 이기적인 이야기라고 보면...</td>\n",
              "      <td>세 커 플 의 그 저 그 런 사 랑 이 야 기 인 것 같 지 만 사 실 한 남 자 ...</td>\n",
              "      <td>커플 사랑 이야기 것 남자 이기 이야기 것</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13626 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            id  ...                                           nn_token\n",
              "0      R009132  ...                                 감독 자유 분방 느낌 모지 이영화\n",
              "1      R045659  ...                            제작사 감독 할복 요청 일본 영화 최대망작\n",
              "2      R015196  ...         드라마 재미 출연자 자 사람 잡 드라마 질 입술 성형 처음 때 지금 잡 서리\n",
              "3      R063595  ...                                초등 아이 때 토마스 다 아이 영화\n",
              "4      R053340  ...                                   중국 미모 연기력 중국 아이콘\n",
              "...        ...  ...                                                ...\n",
              "13621  R002153  ...                                  생각 전 영화 년 전 영화 최고\n",
              "13622  R029129  ...  결말 예측 완전 것 정도 예상 접근 영화 흥미 몰입도 상당 영화 현실 것 고 설명 ...\n",
              "13623  R021090  ...               태 것 공포 영화 중 젤 쓰레기 내용 감정 대응 부분 대 번 볼영\n",
              "13624  R013734  ...  극장 영화 영화 시간 영화 달 동안 여름 밤 낮 새벽 잠 것 촬영 신인감독 신인 배...\n",
              "13625  R050035  ...                            커플 사랑 이야기 것 남자 이기 이야기 것\n",
              "\n",
              "[13626 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28aO4lGrwMjD"
      },
      "source": [
        "### 버트 인풋 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4JGys9DrU4_"
      },
      "source": [
        "https://github.com/monologg/KoBERT-NER 에서 tokenizer 다운"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro5nKW78aktM"
      },
      "source": [
        "import logging\n",
        "import os\n",
        "import unicodedata\n",
        "from shutil import copyfile\n",
        "\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n",
        "                     \"vocab_txt\": \"vocab.txt\"}\n",
        "\n",
        "PRETRAINED_VOCAB_FILES_MAP = {\n",
        "    \"vocab_file\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n",
        "    },\n",
        "    \"vocab_txt\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n",
        "    }\n",
        "}\n",
        "\n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
        "    \"monologg/kobert\": 512,\n",
        "    \"monologg/kobert-lm\": 512,\n",
        "    \"monologg/distilkobert\": 512\n",
        "}\n",
        "\n",
        "PRETRAINED_INIT_CONFIGURATION = {\n",
        "    \"monologg/kobert\": {\"do_lower_case\": False},\n",
        "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n",
        "    \"monologg/distilkobert\": {\"do_lower_case\": False}\n",
        "}\n",
        "\n",
        "SPIECE_UNDERLINE = u'▁'\n",
        "\n",
        "\n",
        "class KoBertTokenizer(PreTrainedTokenizer):\n",
        "    \"\"\"\n",
        "        SentencePiece based tokenizer. Peculiarities:\n",
        "            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n",
        "    \"\"\"\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
        "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_file,\n",
        "            vocab_txt,\n",
        "            do_lower_case=False,\n",
        "            remove_space=True,\n",
        "            keep_accents=False,\n",
        "            unk_token=\"[UNK]\",\n",
        "            sep_token=\"[SEP]\",\n",
        "            pad_token=\"[PAD]\",\n",
        "            cls_token=\"[CLS]\",\n",
        "            mask_token=\"[MASK]\",\n",
        "            **kwargs):\n",
        "        super().__init__(\n",
        "            unk_token=unk_token,\n",
        "            sep_token=sep_token,\n",
        "            pad_token=pad_token,\n",
        "            cls_token=cls_token,\n",
        "            mask_token=mask_token,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "        # Build vocab\n",
        "        self.token2idx = dict()\n",
        "        self.idx2token = []\n",
        "        with open(vocab_txt, 'r', encoding='utf-8') as f:\n",
        "            for idx, token in enumerate(f):\n",
        "                token = token.strip()\n",
        "                self.token2idx[token] = idx\n",
        "                self.idx2token.append(token)\n",
        "\n",
        "        self.max_len_single_sentence = self.max_len - 2  # take into account special tokens\n",
        "        self.max_len_sentences_pair = self.max_len - 3  # take into account special tokens\n",
        "\n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                           \"pip install sentencepiece\")\n",
        "\n",
        "        self.do_lower_case = do_lower_case\n",
        "        self.remove_space = remove_space\n",
        "        self.keep_accents = keep_accents\n",
        "        self.vocab_file = vocab_file\n",
        "        self.vocab_txt = vocab_txt\n",
        "\n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(vocab_file)\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.idx2token)\n",
        "\n",
        "    def __getstate__(self):\n",
        "        state = self.__dict__.copy()\n",
        "        state[\"sp_model\"] = None\n",
        "        return state\n",
        "\n",
        "    def __setstate__(self, d):\n",
        "        self.__dict__ = d\n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                           \"pip install sentencepiece\")\n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(self.vocab_file)\n",
        "\n",
        "    def preprocess_text(self, inputs):\n",
        "        if self.remove_space:\n",
        "            outputs = \" \".join(inputs.strip().split())\n",
        "        else:\n",
        "            outputs = inputs\n",
        "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n",
        "\n",
        "        if not self.keep_accents:\n",
        "            outputs = unicodedata.normalize('NFKD', outputs)\n",
        "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
        "        if self.do_lower_case:\n",
        "            outputs = outputs.lower()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _tokenize(self, text, return_unicode=True, sample=False):\n",
        "        \"\"\" Tokenize a string. \"\"\"\n",
        "        text = self.preprocess_text(text)\n",
        "\n",
        "        if not sample:\n",
        "            pieces = self.sp_model.EncodeAsPieces(text)\n",
        "        else:\n",
        "            pieces = self.sp_model.SampleEncodeAsPieces(text, 128, 0.1)\n",
        "        new_pieces = []\n",
        "        for piece in pieces:\n",
        "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n",
        "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n",
        "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
        "                    if len(cur_pieces[0]) == 1:\n",
        "                        cur_pieces = cur_pieces[1:]\n",
        "                    else:\n",
        "                        cur_pieces[0] = cur_pieces[0][1:]\n",
        "                cur_pieces.append(piece[-1])\n",
        "                new_pieces.extend(cur_pieces)\n",
        "            else:\n",
        "                new_pieces.append(piece)\n",
        "\n",
        "        return new_pieces\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
        "        return self.token2idx.get(token, self.token2idx[self.unk_token])\n",
        "\n",
        "    def _convert_id_to_token(self, index, return_unicode=True):\n",
        "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
        "        return self.idx2token[index]\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n",
        "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n",
        "        return out_string\n",
        "\n",
        "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
        "        by concatenating and adding special tokens.\n",
        "        A RoBERTa sequence has the following format:\n",
        "            single sequence: [CLS] X [SEP]\n",
        "            pair of sequences: [CLS] A [SEP] B [SEP]\n",
        "        \"\"\"\n",
        "        if token_ids_1 is None:\n",
        "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        sep = [self.sep_token_id]\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
        "\n",
        "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
        "        \"\"\"\n",
        "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
        "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
        "        Args:\n",
        "            token_ids_0: list of ids (must not contain special tokens)\n",
        "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
        "                for sequence pairs\n",
        "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
        "                special tokens for the model\n",
        "        Returns:\n",
        "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n",
        "        \"\"\"\n",
        "\n",
        "        if already_has_special_tokens:\n",
        "            if token_ids_1 is not None:\n",
        "                raise ValueError(\n",
        "                    \"You should not supply a second sequence if the provided sequence of \"\n",
        "                    \"ids is already formated with special tokens for the model.\"\n",
        "                )\n",
        "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
        "\n",
        "        if token_ids_1 is not None:\n",
        "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
        "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
        "\n",
        "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
        "        A BERT sequence pair mask has the following format:\n",
        "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
        "        | first sequence    | second sequence\n",
        "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
        "        \"\"\"\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        if token_ids_1 is None:\n",
        "            return len(cls + token_ids_0 + sep) * [0]\n",
        "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
        "\n",
        "    def save_vocabulary(self, save_directory):\n",
        "        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n",
        "            to a directory.\n",
        "        \"\"\"\n",
        "        if not os.path.isdir(save_directory):\n",
        "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
        "            return\n",
        "\n",
        "        # 1. Save sentencepiece model\n",
        "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
        "\n",
        "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n",
        "            copyfile(self.vocab_file, out_vocab_model)\n",
        "\n",
        "        # 2. Save vocab.txt\n",
        "        index = 0\n",
        "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n",
        "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n",
        "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n",
        "                if index != token_index:\n",
        "                    logger.warning(\n",
        "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
        "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n",
        "                    )\n",
        "                    index = token_index\n",
        "                writer.write(token + \"\\n\")\n",
        "                index += 1\n",
        "\n",
        "        return out_vocab_model, out_vocab_txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4Xgl9RtsGul",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151,
          "referenced_widgets": [
            "ece5f7bc097640358632e681b13deff7",
            "2146ba5ff0eb4f4f9597ad9cc5f618d4",
            "a20d986d4dac43188c72e4ce696906ea",
            "ce6a2b06bb574bf0a9e9d41eac9dc0b0",
            "189cee309ae64ea193eb6b30b519f3a9",
            "a7a02f6b3aa4430d9ec0fd9476af0841",
            "07a2eb8b35a74b2d9499f62ddb99f6de",
            "58e48b122d17491794dd4429494a356e",
            "1f8e84c00d8a458b88a13468ff4b45ab",
            "3f56bf234396496cb8f3e75ab08164dc",
            "3e0c5f33cfb940a8a7e0081666dafdbd",
            "da25be21f0374bc988a8262e42fc860a",
            "7709765688674479bb6b7e6094ef3f64",
            "89be3f07490345708d64513881fb7984",
            "bc4e794f5c1a4cc78a33ec6d3ac2433e",
            "bba0b85609a04a4687f6633e04efa983"
          ]
        },
        "outputId": "103ab4a4-0396-4f15-df69-3e9cd90c63c4"
      },
      "source": [
        "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ece5f7bc097640358632e681b13deff7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=371391.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f8e84c00d8a458b88a13468ff4b45ab",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=77779.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Setting 'max_len_single_sentence' is now deprecated. This value is automatically set up.\n",
            "Setting 'max_len_sentences_pair' is now deprecated. This value is automatically set up.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFFokLO0sj_L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9478487a-cb1c-4192-a280-36d408167900"
      },
      "source": [
        "# 문장을 [토큰 인풋, 마스크 인풋, 세그먼트 인풋] 순으로 변환\n",
        "# (bert input 형식으로 문장 변환)\n",
        "\n",
        "def convert_data(data_df):\n",
        "    global tokenizer\n",
        "    \n",
        "    SEQ_LEN = 64 #SEQ_LEN : 버트에 들어갈 인풋의 길이\n",
        "    \n",
        "    tokens, masks, segments, targets = [], [], [], []\n",
        "    \n",
        "    for i in tqdm(range(len(data_df))):\n",
        "        token = tokenizer.encode(data_df[DATA_COLUMN][i], max_length=SEQ_LEN, pad_to_max_length=True)\n",
        "       \n",
        "        num_zeros = token.count(0)\n",
        "        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n",
        "        \n",
        "        segment = [0]*SEQ_LEN\n",
        "\n",
        "        tokens.append(token)\n",
        "        masks.append(mask)\n",
        "        segments.append(segment)\n",
        "        \n",
        "        targets.append(data_df[LABEL_COLUMN][i])\n",
        "\n",
        "    tokens = np.array(tokens)\n",
        "    masks = np.array(masks)\n",
        "    segments = np.array(segments)\n",
        "    targets = np.array(targets)\n",
        "\n",
        "    return [tokens, masks, segments], targets\n",
        "\n",
        "def load_data(pandas_dataframe):\n",
        "    data_df = pandas_dataframe\n",
        "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
        "    data_df[LABEL_COLUMN] = data_df[LABEL_COLUMN].astype(int)\n",
        "    data_x, data_y = convert_data(data_df)\n",
        "    return data_x, data_y\n",
        "\n",
        "\n",
        "# 긍부정 문장을 포함하고 있는 칼럼\n",
        "DATA_COLUMN = \"raw\"\n",
        "# 긍정인지 부정인지를 (1=긍정,0=부정) 포함하고 있는 칼럼\n",
        "LABEL_COLUMN = \"rating\"\n",
        "\n",
        "# train 데이터를 버트 인풋에 맞게 변환\n",
        "train_x, train_y = load_data(train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 54510/54510 [00:11<00:00, 4640.93it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45UY1wudwa3Q"
      },
      "source": [
        "## 모델링"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W85BlYfo2Wge"
      },
      "source": [
        "SEQ_LEN = 64\n",
        "model = TFBertModel.from_pretrained(\"monologg/kobert\", from_pt=True)\n",
        "\n",
        "token_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_word_ids')\n",
        "mask_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_masks')\n",
        "segment_inputs = tf.keras.layers.Input((SEQ_LEN,), dtype=tf.int32, name='input_segment')\n",
        "\n",
        "bert_outputs = model([token_inputs, mask_inputs, segment_inputs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K52Z9my22mJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0ef577f7-693c-48dd-981d-c2829a6860a5"
      },
      "source": [
        "bert_outputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor 'tf_bert_model_3/Identity:0' shape=(None, 64, 768) dtype=float32>,\n",
              " <tf.Tensor 'tf_bert_model_3/Identity_1:0' shape=(None, 768) dtype=float32>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmXsWp-x3mII"
      },
      "source": [
        "bert_outputs = bert_outputs[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiH25UmlK-R2"
      },
      "source": [
        "import tensorflow_addons as tfa\n",
        "opt = tfa.optimizers.RectifiedAdam(lr=5.0e-5, total_steps = 50000, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-08, clipnorm=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5haU2Z687Hmr"
      },
      "source": [
        "sentiment_drop = tf.keras.layers.Dropout(0.8)(bert_outputs)\n",
        "sentiment_first = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))(sentiment_drop)\n",
        "sentiment_model = tf.keras.Model([token_inputs, mask_inputs, segment_inputs], sentiment_first)\n",
        "sentiment_model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4Si9oO33i70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "9e2eaab3-94a6-4da5-c909-ad13f371e5e2"
      },
      "source": [
        "sentiment_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 64)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 64)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_segment (InputLayer)      [(None, 64)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model_3 (TFBertModel)   ((None, 64, 768), (N 92186880    input_word_ids[0][0]             \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 input_segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_155 (Dropout)           (None, 768)          0           tf_bert_model_3[0][1]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1)            769         dropout_155[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 92,187,649\n",
            "Trainable params: 92,187,649\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay9klFVJV3GH"
      },
      "source": [
        "## 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRbbgG3-aB3S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "36832e8c-c2c4-4507-f5a8-a65206dea711"
      },
      "source": [
        "sentiment_model.fit(train_x, train_y, epochs=2, shuffle=True, batch_size=64, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "682/682 [==============================] - 373s 547ms/step - loss: 0.0322 - accuracy: 0.9919 - val_loss: 0.5134 - val_accuracy: 0.8903\n",
            "Epoch 2/2\n",
            "682/682 [==============================] - 371s 543ms/step - loss: 0.0252 - accuracy: 0.9936 - val_loss: 0.4995 - val_accuracy: 0.8885\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4449cfaf28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI0_9hOAwp1-"
      },
      "source": [
        "prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb7LC1YHWW8w"
      },
      "source": [
        "def predict_convert_data(data_df):\n",
        "    global tokenizer\n",
        "    tokens, masks, segments = [], [], []\n",
        "    \n",
        "    for i in tqdm(range(len(data_df))):\n",
        "\n",
        "        token = tokenizer.encode(data_df[DATA_COLUMN][i], max_length=SEQ_LEN, pad_to_max_length=True)\n",
        "        num_zeros = token.count(0)\n",
        "        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n",
        "        segment = [0]*SEQ_LEN\n",
        "\n",
        "        tokens.append(token)\n",
        "        segments.append(segment)\n",
        "        masks.append(mask)\n",
        "\n",
        "    tokens = np.array(tokens)\n",
        "    masks = np.array(masks)\n",
        "    segments = np.array(segments)\n",
        "    return [tokens, masks, segments]\n",
        "    \n",
        "def predict_load_data(pandas_dataframe):\n",
        "    data_df = pandas_dataframe\n",
        "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
        "    data_x = predict_convert_data(data_df)\n",
        "    return data_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzlfYX6xlFEk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e17632e9-dbf6-427c-9385-557b654684f8"
      },
      "source": [
        "test_set = predict_load_data(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 13626/13626 [00:02<00:00, 5210.18it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpMD9Ua_7CNX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "73942212-86b9-40cf-fd96-ee612b536002"
      },
      "source": [
        "test_set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[   2,  790, 1460, ...,    1,    1,    1],\n",
              "        [   2, 4147, 6507, ...,    1,    1,    1],\n",
              "        [   2, 1789, 5760, ..., 6553, 6122,    3],\n",
              "        ...,\n",
              "        [   2, 3298, 7598, ...,    1,    1,    1],\n",
              "        [   2,  517, 5545, ..., 6116, 2355,    3],\n",
              "        [   2, 2801, 4655, ...,    1,    1,    1]]),\n",
              " array([[1, 1, 1, ..., 1, 1, 1],\n",
              "        [1, 1, 1, ..., 1, 1, 1],\n",
              "        [1, 1, 1, ..., 1, 1, 0],\n",
              "        ...,\n",
              "        [1, 1, 1, ..., 1, 1, 1],\n",
              "        [1, 1, 1, ..., 1, 1, 1],\n",
              "        [1, 1, 1, ..., 1, 1, 1]]),\n",
              " array([[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkYBo-dklM8O"
      },
      "source": [
        "preds = sentiment_model.predict(test_set).flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ_8XP0ylOe2"
      },
      "source": [
        "pd.DataFrame({'id':test.id, 'pred':preds}).to_csv('best_kobert_1epoch_prob.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EC7qKDwTWCb"
      },
      "source": [
        "res = []\n",
        "for x in preds:\n",
        "  if x >= 0.5:\n",
        "    res.append(1)\n",
        "  else:\n",
        "    res.append(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shh4AKk7Ti-S"
      },
      "source": [
        "test.id\n",
        "pd.DataFrame({'id':test.id, 'pred':res}).to_csv('best_kobert_1epoch_class.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8JIfi7ZCzrl"
      },
      "source": [
        "## 앙상블"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qus_M8JVzjn1"
      },
      "source": [
        "ko = pd.read_csv('/content/drive/My Drive/딥러닝/감성분석/best_kobert_3epoch_prob.csv')\n",
        "ko_preds = ko.pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vK_duijzvTi"
      },
      "source": [
        "ls = pd.read_csv('/content/drive/My Drive/딥러닝/감성분석/best_LSTM_prob.csv')\n",
        "ls_preds = ls.pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0ZVshr6DIGO"
      },
      "source": [
        "#cnn = pd.read_csv('/content/drive/My Drive/딥러닝/감성분석/best_cnn_prob.csv')\n",
        "#cnn_preds = cnn.pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPCeMXtrz1pK"
      },
      "source": [
        "GM = (ko_preds * ls_preds) ** (1/2) # 기하 평균 Geometric Mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc6fJPDgW-eQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "b7adbc0f-dd96-45d1-b7b8-b46ae4c54c4f"
      },
      "source": [
        "GM"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        0.012864\n",
              "1        0.018670\n",
              "2        0.005467\n",
              "3        0.986147\n",
              "4        0.781612\n",
              "           ...   \n",
              "13621    0.958113\n",
              "13622    0.026518\n",
              "13623    0.016730\n",
              "13624    0.023093\n",
              "13625    0.948271\n",
              "Name: pred, Length: 13626, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUFvTwED0Swk"
      },
      "source": [
        "gm_res = []\n",
        "for x in GM:\n",
        "  if x >= 0.5:\n",
        "    gm_res.append(1)\n",
        "  else:\n",
        "    gm_res.append(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z92gYs20Z0f"
      },
      "source": [
        "pd.DataFrame({'id':test.id, 'pred':gm_res}).to_csv('best_kobert_lstm_gmean3.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}